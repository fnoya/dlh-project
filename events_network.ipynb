{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "african-geography",
   "metadata": {},
   "source": [
    "Install OS dependencies.  This only needs to be run once for each new notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-hygiene",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "printable-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\r\n",
    "import numpy as np\r\n",
    "import string\r\n",
    "import datetime\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "import random\r\n",
    "import math\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.utils.data.dataset import random_split\r\n",
    "from torch.utils.data import DataLoader, Subset\r\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb')) + 1\n",
    "assert max_code==127, \"EVENTS MAX CODE changed?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_max_visits = 505"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-atmosphere",
   "metadata": {},
   "source": [
    "Setting the CUDA device, if no cuda, we will use CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-survey",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n\nNVIDIA GeForce GTX 1060 6GB\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-charge",
   "metadata": {},
   "source": [
    "The following code is to enable us to import data from athena services, be able to query parquet files through SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-transcription",
   "metadata": {},
   "source": [
    "First we will need a function to load the pre-processed train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events_dataset_object(prefix=''):\n",
    "    return pickle.load( open(prefix + \"events_item.p\", \"rb\" )), pickle.load(open(prefix + \"events_value.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pediatric-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patient_mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesEventsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, notes_mask, events_items, events_values, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        len_events_patients = len(events_items['subject_id'].unique())\n",
    "        self.x = patients_notes.to(device, non_blocking=True)\n",
    "        self.notes_mask = notes_mask.to(device, non_blocking=True)\n",
    "        self.y = mortality.to(device, non_blocking=True)\n",
    "        self.items = events_items.groupby('subject_id').agg('codes').apply(list).values\n",
    "        self.values = events_values.groupby('subject_id').agg('values').apply(list).values\n",
    "        assert len(self.x) == len_events_patients, 'Notes patients and events patients counts do not match!'\n",
    "        r = random.randrange(len(self.x))\n",
    "        assert events_items['subject_id'].unique()[r] == self.patient_id[r], 'Notes and events patient id=' + str(r) + ' does not match'\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        events = np.zeros([len(self.items[index]), max_code])\n",
    "\n",
    "        for i, codes in enumerate(self.items[index]):\n",
    "            for j, code in enumerate(codes):\n",
    "                v = self.values[index][i][j]\n",
    "                events[i, code] = v if not math.isnan(v) else 0.0\n",
    "        \n",
    "\n",
    "        return(self.x[index].to_dense(), self.notes_mask[index].to_dense(), events, self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (cohort_type = 'original'):\n",
    "    \"\"\"\n",
    "    cohort_type = 'original' -> Unbalanced cohort will be created\n",
    "    cohort_type = 'essential'  ->  Unbalanced cohort with just the minimum set of events features\n",
    "    cohort_type = 'balanced_train' -> Balanced cohort for training will be created\n",
    "    cohort_type = 'balanced_test' -> Balanced cohort for testing will be created\n",
    "    \"\"\"\n",
    "    notes_prefix = \"orig_\" if cohort_type in ['original','essential'] else \"train_\" if cohort_type == 'balanced_train' else \"test_\"\n",
    "    subject_id, patients_notes_fetures, index_0, index_1, patient_mortality= load_notes_dataset_object(prefix = notes_prefix)\n",
    "    index = [index_0, index_1]\n",
    "    patients_notes_fetures = torch.sparse_coo_tensor(index, patients_notes_fetures, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    ones = np.ones((len(index_0),200))\n",
    "    notes_mask = torch.sparse_coo_tensor(index, ones, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    patient_mortality = torch.from_numpy(patient_mortality).float()    \n",
    "    \n",
    "    events_prefix = \"\" if cohort_type=='original' else cohort_type +\"_\" \n",
    "    events_items, events_values = load_events_dataset_object(events_prefix)\n",
    "\n",
    "    assert len(events_items)==len(events_values) and len(events_values['subject_id'].unique()) == len(events_items['subject_id'].unique()) == len(patient_mortality) , \"Wrong events dataframes?\"\n",
    "        \n",
    "    dataset = NotesEventsDataset(subject_id, patients_notes_fetures, notes_mask, events_items, events_values, patient_mortality)\n",
    "    assert len(patient_mortality) == len(dataset), 'Wrong dataset length!'\n",
    "    print (\"Number of Patients:\", len(patient_mortality))\n",
    "\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    x, notes_mask, events, mortality_flag = zip(*data)\n",
    "    \n",
    "    maxvisits = max([len(p) for p in events])\n",
    "    \n",
    "    events_result = torch.tensor([np.concatenate((p, np.zeros([maxvisits - len(p), max_code]))) for p in events]).float()\n",
    "    events_mask = torch.tensor([np.concatenate((np.ones(len(p)), np.zeros(maxvisits - len(p)))) for p in events]).int()\n",
    "    x = torch.stack(x)\n",
    "    notes_mask = torch.stack(notes_mask)\n",
    "    mortality_flag = torch.stack(mortality_flag)\n",
    "    events_result = events_result.to(device, non_blocking=True)\n",
    "    events_mask = events_mask.to(device, non_blocking=True)\n",
    "    return x, notes_mask, events_result, events_mask, mortality_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbalanced_dataloaders (max_size = 0):\n",
    "\n",
    "    dataset = create_dataset('original')\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        dataset = Subset(dataset, np.arange(max_size))\n",
    "\n",
    "    split = int(len(dataset)*0.8)\n",
    "    lengths = [split, len(dataset) - split]\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_dataloaders (max_size = 0):\n",
    "\n",
    "    print (\"* Train dataset *\")\n",
    "    balanced_train_dataset = create_dataset('balanced_train')\n",
    "    print (\"* Test dataset *\")\n",
    "    balanced_test_dataset = create_dataset('balanced_test')\n",
    "\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        balanced_train_dataset = Subset(balanced_train_dataset, np.arange(max_size))\n",
    "        balanced_test_dataset = Subset(balanced_test_dataset, np.arange(max_size))\n",
    "\n",
    "    train_loader = DataLoader(balanced_train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(balanced_test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Alpha attention mechanism to compute the attention weights corresponding to each date with events data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden layer dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"        \n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.softmax(self.a_att(g), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Beta attention mechanism to compute the attention weights corresponding to each event code.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_dim: the hidden layer dimension\n",
    "            emb_dim: the number of events codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(input_dim, emb_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, input_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, # of events codes)\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.tanh(self.b_att(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventsRNN(nn.Module):\n",
    "    \n",
    "    def attention_sum(self, alpha, beta, x, masks):\n",
    "        \"\"\"\n",
    "            Performs the weighted sum of the events data using alpha and beta attention weights. \n",
    "            It also sets to 0 the positions corresponding to dates without events data using the masks information.\n",
    "\n",
    "        Arguments:\n",
    "            alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "            beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            x: the events data for each date with shape (batch_size, # of dates, # of events codes)\n",
    "            masks: the padding masks in time of shape (batch_size, # of dates, # of events codes)\n",
    "\n",
    "        Outputs:\n",
    "            c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        masks = masks.unsqueeze(-1)\n",
    "        return torch.sum( beta * x * alpha * masks , dim=1 )\n",
    "\n",
    "\n",
    "    def __init__(self, num_codes, emb_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the RNN-alpha using `nn.GRU()`\n",
    "        self.rnn_a = nn.GRU(num_codes, 128, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`\n",
    "        self.rnn_b = nn.GRU(num_codes, 128, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`\n",
    "        self.att_a = AlphaAttention(128)\n",
    "        # Define the beta-attention using `BetaAttention()`\n",
    "        self.att_b = BetaAttention(128, num_codes)\n",
    "        # Define the linear layers using `nn.Linear()`\n",
    "        self.fc = nn.Linear(num_codes, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, events, masks):\n",
    "\n",
    "        # Pass the events data through RNN-alpha\n",
    "        g, _ = self.rnn_a(events)\n",
    "        # Pass the events data through RNN-beta\n",
    "        h, _ = self.rnn_b(events)\n",
    "        # Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        # Perform the weighted sum of the events data using the attention weights for the dates with events data\n",
    "        c = self.attention_sum(alpha, beta, events, masks)\n",
    "        # Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer():\n",
    "    model = EventsRNN(max_code, emb_size=128)\n",
    "    if torch.cuda.device_count() >0:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, nesterov = True)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "            x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(events, events_masks)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(events, events_masks)\n",
    "            val_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().cpu().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, val_loader, n_epochs=10, filename='model.pt'):\n",
    "    t0 = time.time()\n",
    "    train(model, train_loader, n_epochs)\n",
    "    t1 = time.time()\n",
    "    processing_time = t1-t0\n",
    "    print('Model Training time: ' + str(processing_time))\n",
    "    \n",
    "    p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print (\"Learning rate: \" + str(learning_rate))\n",
    "    print(\"Model Training time: \" + str(processing_time))\n",
    "    print(\"Precision = \",p)\n",
    "    print(\"Recall    = \", r)\n",
    "    print(\"F1        = \", f)\n",
    "    print(\"ROC AUC   = \", roc_auc)\n",
    "    print(p,\"\\t\",r,\"\\t\",f,\"\\t\",roc_auc)\n",
    "    \"\"\"\n",
    "    if filename is not None:\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    \"\"\"\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 0.001\n",
      "Number of Epochs: 10\n",
      "\n",
      "--------------\n",
      "Original model\n",
      "--------------\n",
      "Number of Patients: 9822\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 0: curr_epoch_loss=0.429216206073761\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 1: curr_epoch_loss=0.2886306941509247\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 2: curr_epoch_loss=0.27539151906967163\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 3: curr_epoch_loss=0.23904095590114594\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 4: curr_epoch_loss=0.23107142746448517\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 5: curr_epoch_loss=0.22080597281455994\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 6: curr_epoch_loss=0.22208595275878906\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 7: curr_epoch_loss=0.21734042465686798\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 8: curr_epoch_loss=0.2024022340774536\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 9: curr_epoch_loss=0.19729465246200562\n",
      "Model Training time: 278.23018860816956\n",
      "Learning rate: 0.001\n",
      "Model Training time: 278.23018860816956\n",
      "Precision =  0.6032608695652174\n",
      "Recall    =  0.46443514644351463\n",
      "F1        =  0.524822695035461\n",
      "ROC AUC   =  0.8506087066135938\n",
      "0.6032608695652174 \t 0.46443514644351463 \t 0.524822695035461 \t 0.8506087066135938\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6032608695652174,\n",
       " 0.46443514644351463,\n",
       " 0.524822695035461,\n",
       " 0.8506087066135938)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Original model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_unbalanced_dataloaders()   # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'unbalanced_model.pt')\n",
    "#load_and_eval(model, 'unbalanced_model.pt', val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 0.0001\n",
      "Number of Epochs: 10\n",
      "\n",
      "\n",
      "--------------\n",
      "Balanced model\n",
      "--------------\n",
      "* Train dataset *\n",
      "Number of Patients: 13790\n",
      "* Test dataset *\n",
      "Number of Patients: 1965\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 0: curr_epoch_loss=0.6428784728050232\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 1: curr_epoch_loss=0.46494531631469727\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 2: curr_epoch_loss=0.3550058901309967\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 3: curr_epoch_loss=0.28842878341674805\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 4: curr_epoch_loss=0.23932117223739624\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 5: curr_epoch_loss=0.20136545598506927\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 6: curr_epoch_loss=0.17096295952796936\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 7: curr_epoch_loss=0.1442098617553711\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 8: curr_epoch_loss=0.12268555909395218\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 9: curr_epoch_loss=0.10383887588977814\n",
      "Model Training time: 644.5986795425415\n",
      "Learning rate: 0.0001\n",
      "Model Training time: 644.5986795425415\n",
      "Precision =  0.7318840579710145\n",
      "Recall    =  0.8632478632478633\n",
      "F1        =  0.792156862745098\n",
      "ROC AUC   =  0.9600473023349972\n",
      "0.7318840579710145 \t 0.8632478632478633 \t 0.792156862745098 \t 0.9600473023349972\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7318840579710145, 0.8632478632478633, 0.792156862745098, 0.9600473023349972)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "print ('')\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Balanced model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_balanced_dataloaders()       # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'balanced_model.pt')\n",
    "#load_and_eval(model, 'balanced_model.pt', val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}