{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "african-geography",
   "metadata": {},
   "source": [
    "Install OS dependencies.  This only needs to be run once for each new notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-marketplace",
   "metadata": {},
   "source": [
    "!pip install PyAthena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-objective",
   "metadata": {},
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-hygiene",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "printable-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string\n",
    "# import nltk\n",
    "#from nltk import word_tokenize\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import ClientError\n",
    "# below is used to print out pretty pandas dataframes\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "from pyathena import connect\n",
    "from pyathena.pandas.util import as_pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-atmosphere",
   "metadata": {},
   "source": [
    "Setting the CUDA device, if no cuda, we will use CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "realistic-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-charge",
   "metadata": {},
   "source": [
    "The following code is to enable us to import data from athena services, be able to query parquet files through SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retired-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3 = boto3.resource('s3')\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "athena_query_results_bucket = 'aws-athena-query-results-'+account_id+'-'+region\n",
    "\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=athena_query_results_bucket)\n",
    "except ClientError:\n",
    "    bucket = s3.create_bucket(Bucket=athena_query_results_bucket)\n",
    "    print('Creating bucket '+athena_query_results_bucket)\n",
    "cursor = connect(s3_staging_dir='s3://'+athena_query_results_bucket+'/athena/temp').cursor()\n",
    "\n",
    "\n",
    "# The Glue database name of your MIMIC-III parquet data\n",
    "#gluedatabase=\"mimiciii\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-impression",
   "metadata": {},
   "source": [
    "We have already pre-processed our cohort set using SQL queries, the following is the query to fetch the list of all cohort patients and then process the notes for each patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-repository",
   "metadata": {},
   "source": [
    "query = 'select cohort.subject_id, cohort.mortality_flag from default.diabetic_patients_cohort cohort order by cohort.subject_id limit 1000'\n",
    "cursor.execute(query)\n",
    "cohort_patients_df = as_pandas(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-seeking",
   "metadata": {},
   "source": [
    "cohort_patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-respect",
   "metadata": {},
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to find what is the maximum number of visit dates for any given patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "query = 'select max(number_dates) from (select nts.subject_id, count(nts.chart_date) as number_dates from default.diabetic_patients_notes_agg nts group by nts.subject_id)'\n",
    "cursor.execute(query)\n",
    "patients_max_visits = as_pandas(cursor)\n",
    "patients_max_visits = patients_max_visits.values[0][0]\n",
    "print(patients_max_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-maker",
   "metadata": {},
   "source": [
    "The following function is to pre-process the notes, get rid of numbers, punctuation and tokenize the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-insight",
   "metadata": {},
   "source": [
    "def preprocess_dataset(df):    \n",
    "    ''' Preprocess the text data. And return a list of clinical notes. '''\n",
    "    clinical_notes = []\n",
    "    \n",
    "    df.notes_agg = df.notes_agg.fillna(' ')  # remove NA\n",
    "    df.notes_agg = df.notes_agg.str.replace('\\n',' ')  # remove newline\n",
    "    df.notes_agg = df.notes_agg.str.replace('\\r',' ')\n",
    "    \"\"\"\n",
    "    TODO: 1. remove punc;\n",
    "          2. remove numbers.\n",
    "          \n",
    "    HINT: consider using `string.punctuation`, `str.maketrans`, and `str.translate`.\n",
    "    \"\"\"\n",
    "    df.notes_agg = df.notes_agg.str.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    df.notes_agg = df.notes_agg.str.translate(str.maketrans('', '', '1234567890')) # remove numbers\n",
    "    \n",
    "    df.notes_agg = df.notes_agg.str.lower()  # convert to lower case\n",
    "    \n",
    "    # tokenize\n",
    "    for note in df.notes_agg.values:\n",
    "        note_tokenized = word_tokenize(note)\n",
    "        clinical_notes.append(note_tokenized)\n",
    "\n",
    "    return clinical_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-mistake",
   "metadata": {},
   "source": [
    "To process our notes, we first need to load our pre-trained word2vec model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-attention",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "#pubMedWord2VecModel = KeyedVectors.load_word2vec_format('PubMed-w2v.bin', binary=True)\n",
    "word2vec_model = KeyedVectors.load('note_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-vermont",
   "metadata": {},
   "source": [
    "#word2vec_model.get_vector('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-telescope",
   "metadata": {},
   "source": [
    "Now we can start processing the notes for each patient:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-acrylic",
   "metadata": {},
   "source": [
    "t0 = time.time()\n",
    "\n",
    "patients_notes_fetures = torch.zeros(len(cohort_patients_df), patients_max_visits,200, dtype = torch.float)\n",
    "patients_notes_fetures_mask = torch.zeros(len(cohort_patients_df), 1, dtype = torch.long)\n",
    "missing_words = 0\n",
    "patient_subject_id = []\n",
    "\n",
    "query = ('select nts.subject_id, nts.chart_date, nts.notes_agg from default.diabetic_patients_notes_agg nts '\n",
    "         'join (select cohort.subject_id, cohort.mortality_flag from default.diabetic_patients_cohort cohort order by cohort.subject_id limit 1000) cohort '\n",
    "         'on nts.subject_id = cohort.subject_id '\n",
    "         'order by nts.subject_id asc, nts.chart_date asc;')\n",
    "cursor.execute(query)\n",
    "patients_date_notes = as_pandas(cursor)\n",
    "    \n",
    "for patient_idx, patient in enumerate(cohort_patients_df.subject_id.values):\n",
    "    patient_subject_id.append(patient)\n",
    "    patient_date_notes_list = preprocess_dataset(patients_date_notes[patients_date_notes.subject_id == patient].copy())\n",
    "    #print(patient_date_notes_list)\n",
    "    for date_idx, note in enumerate(patient_date_notes_list):\n",
    "        patient_date_note = torch.zeros(200)\n",
    "        #print(note)\n",
    "        for note_word in note:\n",
    "            #if note_word not in vocab:\n",
    "                #vocab.append(note_word)\n",
    "            try:\n",
    "                patient_date_note = patient_date_note + torch.FloatTensor(word2vec_model.get_vector(note_word))\n",
    "            except:\n",
    "                missing_words = missing_words + 1\n",
    "                #if note_word not in missing_words:\n",
    "                    #missing_words =  missing_words+1\n",
    "        patients_notes_fetures[patient_idx][date_idx][:] = patient_date_note\n",
    "        patients_notes_fetures_mask[patient_idx][0] = date_idx \n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-nutrition",
   "metadata": {},
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-trader",
   "metadata": {},
   "source": [
    "print(str(patients_notes_fetures.element_size() * patients_notes_fetures.nelement()/1024/1024/1024) + ' GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-catholic",
   "metadata": {},
   "source": [
    "print(missing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-speaking",
   "metadata": {},
   "source": [
    "print(patients_notes_fetures.shape)\n",
    "print(len(patients_notes_fetures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-count",
   "metadata": {},
   "source": [
    "#print(patients_notes_fetures[1][0])\n",
    "#print(patients_notes_fetures_mask[0][4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-capitol",
   "metadata": {},
   "source": [
    "patients_mortality = torch.FloatTensor(cohort_patients_df.mortality_flag.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-transcription",
   "metadata": {},
   "source": [
    "First we will need a function to load the pre-processed train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pediatric-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patients_notes_last_date = np.load(prefix + 'patients_notes_last_date.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patients_notes_last_date, patient_mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-sentence",
   "metadata": {},
   "source": [
    "We now load the objects we for train and test dataset and create the notes features sparse tensor for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "helpful-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subject_id, train_patients_notes_fetures, train_index_0, train_index_1, train_patients_notes_last_date, train_patient_mortality = load_notes_dataset_object(prefix = 'train_')\n",
    "train_index = [train_index_0, train_index_1]\n",
    "train_patients_notes_fetures = torch.sparse_coo_tensor(train_index, train_patients_notes_fetures, (len(train_subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "train_patients_notes_last_date = torch.from_numpy(train_patients_notes_last_date).long()\n",
    "train_patient_mortality = torch.from_numpy(train_patient_mortality).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "indonesian-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_id, test_patients_notes_fetures, test_index_0, test_index_1, test_patients_notes_last_date, test_patient_mortality = load_notes_dataset_object(prefix = 'test_')\n",
    "test_index = [test_index_0, test_index_1]\n",
    "test_patients_notes_fetures = torch.sparse_coo_tensor(test_index, test_patients_notes_fetures, (len(test_subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "test_patients_notes_last_date = torch.from_numpy(test_patients_notes_last_date).long()\n",
    "test_patient_mortality = torch.from_numpy(test_patient_mortality).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "devoted-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5],\n",
      "        [16],\n",
      "        [10],\n",
      "        ...,\n",
      "        [10],\n",
      "        [50],\n",
      "        [ 6]])\n"
     ]
    }
   ],
   "source": [
    "#print(test_patients_notes_last_date[0:50])\n",
    "print(train_patients_notes_last_date)\n",
    "#.expand(-1,200)\n",
    "#print(last_visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-flush",
   "metadata": {},
   "source": [
    "Now we are going to create a custom notes dataset to then partition the data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "selective-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NotesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, last_date_idx, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        self.x = patients_notes\n",
    "        self.mask = last_date_idx\n",
    "        self.y = mortality\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return((self.patient_id , self.x[index].to_dense(), self.mask[index], self.y[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "supported-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_train_dataset = NotesDataset(train_subject_id, train_patients_notes_fetures, train_patients_notes_last_date, train_patient_mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "respective-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_val_dataset = NotesDataset(test_subject_id, test_patients_notes_fetures, test_patients_notes_last_date, test_patient_mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "valued-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 13790\n",
      "Length of val dataset: 1965\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset:\", len(notes_train_dataset))\n",
    "print(\"Length of val dataset:\", len(notes_val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-database",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(230729)\n",
    "\n",
    "split = int(len(notes_Dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(notes_Dataset) - split]\n",
    "notes_train_dataset, notes_val_dataset = random_split(notes_Dataset, lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "minute-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 50\n",
    "notes_train_loader = DataLoader(notes_train_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "complete-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_val_loader = DataLoader(notes_val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "tracked-interference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(notes_train_loader))\n",
    "print(len(notes_val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-artist",
   "metadata": {},
   "source": [
    "Since the number of date_notes is not the same for each patient, we need to get the hidden state for the last note date for each patient, for that we implement the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "standing-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_note_date(hidden_states, masks):   \n",
    "    #last_visit = ((masks.sum(axis = 2) > 0).sum(axis = 1) - 1).unsqueeze(-1)\n",
    "    #if(step == 134):\n",
    "    #print(masks)\n",
    "    #print(hidden_states.shape)\n",
    "    last_visit = masks.expand(-1,hidden_states.shape[2]).unsqueeze(1)\n",
    "    \n",
    "    out = torch.gather(hidden_states,dim = 1,index = last_visit)[:,-1,:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ideal-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_volume(W, K, S, P):\n",
    "    \n",
    "\n",
    "    \n",
    "    return  (((W-K+2*P)//S)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-government",
   "metadata": {},
   "source": [
    "Now We can proceed to create our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "confused-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, notes_emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_size = notes_emb_size\n",
    "        self.RNN = nn.GRU(input_size = notes_emb_size, hidden_size = notes_emb_size, batch_first = True)\n",
    "        self.fc1 = nn.Linear(notes_emb_size,notes_emb_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(notes_emb_size,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, step):\n",
    "                \n",
    "        rnn_out = self.RNN(x)\n",
    "        last_note_date_hs = get_last_note_date(rnn_out[0],masks)\n",
    "        fc1_out = self.fc1(last_note_date_hs)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        dp_out = self.dropout(fc1_out)\n",
    "        fc2_out = self.fc2(dp_out)\n",
    "        out = self.sig(fc2_out).flatten()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "assumed-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_rnn = NotesRNN(notes_emb_size = 200)\n",
    "if torch.cuda.device_count() >0:\n",
    "    notes_rnn.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(notes_rnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "second-rugby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "empirical-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        subject_id, x, masks, labels = batch\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks,0)\n",
    "            val_labels.extend(labels.detach().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary' )\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "industrial-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "                #print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "            subject_id, x, masks, labels = batch\n",
    "            \"\"\"Pushing tensors to CUDA\"\"\"\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(x, masks, step)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "current-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 0: curr_epoch_loss=0.4811288118362427\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 1: curr_epoch_loss=0.6054174304008484\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 2: curr_epoch_loss=0.5302814245223999\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 3: curr_epoch_loss=0.49054014682769775\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 4: curr_epoch_loss=0.45929083228111267\n",
      "Model Training time: 1301.986031293869\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "t0 = time.time()\n",
    "n_epochs = 5\n",
    "train(notes_rnn, notes_train_loader, notes_val_loader, n_epochs)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Model Training time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "interested-plasma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1323529411764706 1.0 0.23376623376623376 0.8351219837355018\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(notes_rnn, notes_val_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-serum",
   "metadata": {},
   "source": [
    "Trying with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "finite-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_subject_id, orig_patients_notes_fetures, orig_index_0, orig_index_1, orig_patients_notes_last_date, orig_patient_mortality = load_notes_dataset_object(prefix = 'orig_')\n",
    "orig_index = [orig_index_0, orig_index_1]\n",
    "orig_patients_notes_fetures = torch.sparse_coo_tensor(orig_index, orig_patients_notes_fetures, (len(orig_subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "orig_patients_notes_last_date = torch.from_numpy(orig_patients_notes_last_date).long()\n",
    "orig_patient_mortality = torch.from_numpy(orig_patient_mortality).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "competent-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_orig_dataset = NotesDataset(orig_subject_id, orig_patients_notes_fetures, orig_patients_notes_last_date, orig_patient_mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "liberal-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(230729)\n",
    "split = int(len(notes_orig_dataset)*0.8)\n",
    "lengths = [split, len(notes_orig_dataset) - split]\n",
    "\n",
    "notes_orig_train_dataset, notes_orig_val_dataset = random_split(notes_orig_dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ranking-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_orig_train_loader = DataLoader(notes_orig_train_dataset, batch_size=batch_size)\n",
    "notes_orig_val_loader = DataLoader(notes_orig_val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "formed-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_rnn = NotesRNN(notes_emb_size = 200)\n",
    "if torch.cuda.device_count() >0:\n",
    "    notes_rnn.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(notes_rnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "earlier-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 0: curr_epoch_loss=0.4201892912387848\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 1: curr_epoch_loss=0.3786067068576813\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 2: curr_epoch_loss=0.36374491453170776\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 3: curr_epoch_loss=0.34486886858940125\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 4: curr_epoch_loss=0.32579171657562256\n",
      "Model Training time: 688.6786022186279\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "t0 = time.time()\n",
    "n_epochs = 5\n",
    "train(notes_rnn, notes_orig_train_loader, notes_orig_val_loader, n_epochs)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Model Training time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "committed-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4090909090909091 0.04186046511627907 0.0759493670886076 0.6922245847176081\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(notes_rnn, notes_orig_val_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "fuzzy-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, notes_emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_size = notes_emb_size\n",
    "        self.RNN = nn.GRU(input_size = notes_emb_size, hidden_size = notes_emb_size, batch_first = True)\n",
    "        self.attention = nn.Linear(505,505)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(505,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, step):\n",
    "                \n",
    "        rnn_out = self.RNN(x)\n",
    "        sum_hidden_states = rnn_out[0].sum(dim = 2)\n",
    "        attention_out = self.attention(sum_hidden_states)\n",
    "        attention_out = self.relu(attention_out)\n",
    "        dp_out = self.dropout(attention_out)\n",
    "        fc2_out = self.fc2(dp_out)\n",
    "        out = self.sig(fc2_out).flatten()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "random-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_rnn = NotesRNN(notes_emb_size = 200)\n",
    "if torch.cuda.device_count() >0:\n",
    "    notes_rnn.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(notes_rnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "sonic-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 0: curr_epoch_loss=0.42356574535369873\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 1: curr_epoch_loss=0.37926116585731506\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 2: curr_epoch_loss=0.3613307774066925\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 3: curr_epoch_loss=0.3446982204914093\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 4: curr_epoch_loss=0.3264455199241638\n",
      "Model Training time: 713.2944731712341\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "t0 = time.time()\n",
    "n_epochs = 5\n",
    "train(notes_rnn, notes_orig_train_loader, notes_orig_val_loader, n_epochs)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Model Training time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "partial-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32142857142857145 0.04186046511627907 0.07407407407407408 0.6954803986710963\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(notes_rnn, notes_orig_val_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-boring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
