{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "devoted-hygiene",
   "metadata": {},
   "source": [
    "# Notes Network\n",
    "This notebook runs the Notes Network by itself, to validate the contribution of the Network when we train and test based on this network alone."
   ]
  },
  {
   "source": [
    "Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "printable-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\r\n",
    "import numpy as np\r\n",
    "import string\r\n",
    "import datetime\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "import random\r\n",
    "import math\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.utils.data.dataset import random_split\r\n",
    "from torch.utils.data import DataLoader, Subset\r\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "source": [
    "## Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb')) + 1\n",
    "assert max_code==127, \"EVENTS MAX CODE changed?\""
   ]
  },
  {
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to find what is the maximum number of visit dates for any given patient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_max_visits = 505"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-atmosphere",
   "metadata": {},
   "source": [
    "In preparation to run the models training on CUDA, we need to make sure that we do have a device and load the tensors and model to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-survey",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n\nNVIDIA GeForce GTX 1060 6GB\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-transcription",
   "metadata": {},
   "source": [
    "## Loading data from files\n",
    "\n",
    "These files were generated from SQL queries contained in notes_extraction.ipynb and events_extraction.ipynb notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pediatric-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    \n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patient_mortality"
   ]
  },
  {
   "source": [
    "## Dataset Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesEventsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, notes_mask, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        self.x = patients_notes.to(device, non_blocking=True)\n",
    "        self.notes_mask = notes_mask.to(device, non_blocking=True)\n",
    "        self.y = mortality.to(device, non_blocking=True)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "\n",
    "        return(self.x[index].to_dense(), self.notes_mask[index].to_dense(), self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (cohort_type = 'original'):\n",
    "    \"\"\"\n",
    "    cohort_type = 'original' -> Unbalanced cohort will be created\n",
    "    cohort_type = 'essential'  ->  Unbalanced cohort with just the minimum set of events features\n",
    "    cohort_type = 'balanced_train' -> Balanced cohort for training will be created\n",
    "    cohort_type = 'balanced_test' -> Balanced cohort for testing will be created\n",
    "    \"\"\"\n",
    "    notes_prefix = \"orig_\" if cohort_type in ['original','essential'] else \"train_\" if cohort_type == 'balanced_train' else \"test_\"\n",
    "    subject_id, patients_notes_fetures, index_0, index_1, patient_mortality= load_notes_dataset_object(prefix = notes_prefix)\n",
    "    index = [index_0, index_1]\n",
    "    patients_notes_fetures = torch.sparse_coo_tensor(index, patients_notes_fetures, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    ones = np.ones((len(index_0),200))\n",
    "    notes_mask = torch.sparse_coo_tensor(index, ones, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    patient_mortality = torch.from_numpy(patient_mortality).float()    \n",
    "    \n",
    "    \n",
    "        \n",
    "    dataset = NotesEventsDataset(subject_id, patients_notes_fetures, notes_mask, patient_mortality)\n",
    "    \n",
    "    print (\"Number of Patients:\", len(patient_mortality))\n",
    "\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "source": [
    "## Dataloaders Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "source": [
    "### Dataloader for the unbalanced (original) cohort."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbalanced_dataloaders (max_size = 0):\n",
    "\n",
    "    dataset = create_dataset('original')\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        dataset = Subset(dataset, np.arange(max_size))\n",
    "\n",
    "    split = int(len(dataset)*0.8)\n",
    "    lengths = [split, len(dataset) - split]\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "source": [
    "### Dataloader for the balanced cohort (data up-sampling) for \"dead\" patients to eliminate class imbalance)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_dataloaders (max_size = 0):\n",
    "\n",
    "    print (\"* Train dataset *\")\n",
    "    balanced_train_dataset = create_dataset('balanced_train')\n",
    "    print (\"* Test dataset *\")\n",
    "    balanced_test_dataset = create_dataset('balanced_test')\n",
    "\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        balanced_train_dataset = Subset(balanced_train_dataset, np.arange(max_size))\n",
    "        balanced_test_dataset = Subset(balanced_test_dataset, np.arange(max_size))\n",
    "\n",
    "    train_loader = DataLoader(balanced_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(balanced_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "source": [
    "## Model Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Alpha Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesAlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\"\n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = self.a_att(g)\n",
    "        alpha = torch.softmax(weights,1)\n",
    "        return alpha"
   ]
  },
  {
   "source": [
    "### Beta Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesBetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        weights = self.b_att(h)\n",
    "        beta = torch.tanh(weights)\n",
    "        return beta"
   ]
  },
  {
   "source": [
    "### Notes Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesRNN(nn.Module):\n",
    "    \n",
    "    def attention_sum(self, alpha, beta, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "            beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        #masks = (torch.sum(masks, 2) > 0).type(torch.float).unsqueeze(2)\n",
    "        \n",
    "        return torch.sum( x * alpha * beta * masks , dim=1 )\n",
    "    \n",
    "    def __init__(self, hidden_dim=128, notes_emb_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_a = nn.GRU(notes_emb_size, notes_emb_size, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(notes_emb_size, notes_emb_size, batch_first=True)\n",
    "        self.att_a = NotesAlphaAttention(notes_emb_size)\n",
    "        self.att_b = NotesBetaAttention(notes_emb_size)\n",
    "        self.fc = nn.Linear(notes_emb_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        g, _ = self.rnn_a(x)\n",
    "        h, _ = self.rnn_b(x)\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        \n",
    "        \n",
    "        c = self.attention_sum(alpha, beta, x, masks)\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "        return probs.squeeze()"
   ]
  },
  {
   "source": [
    "## Model training and evaluation Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "            x, masks, labels = batch\n",
    "        \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(x, masks)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks)\n",
    "            val_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().cpu().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, val_loader, n_epochs=10, filename='model.pt'):\n",
    "    t0 = time.time()\n",
    "    train(model, train_loader, n_epochs)\n",
    "    t1 = time.time()\n",
    "    processing_time = t1-t0\n",
    "    print('Model Training time: ' + str(processing_time))\n",
    "    \n",
    "    p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print (\"Learning rate: \" + str(learning_rate))\n",
    "    print(\"Model Training time: \" + str(processing_time))\n",
    "    print(\"Precision = \",p)\n",
    "    print(\"Recall    = \", r)\n",
    "    print(\"F1        = \", f)\n",
    "    print(\"ROC AUC   = \", roc_auc)\n",
    "    print(p,\"\\t\",r,\"\\t\",f,\"\\t\",roc_auc)\n",
    "    \n",
    "    '''\n",
    "    if filename is not None:\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    '''\n",
    "    return p, r, f, roc_auc\n"
   ]
  },
  {
   "source": [
    "## Model Creation and Optimizer definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer():\n",
    "    model = NotesRNN(hidden_dim=128)\n",
    "    if torch.cuda.device_count() >0:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, nesterov = True)\n",
    "    return model, optimizer"
   ]
  },
  {
   "source": [
    "## Model Training and Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 0.001\n",
      "Number of Epochs: 10\n",
      "\n",
      "--------------\n",
      "Original model\n",
      "--------------\n",
      "Number of Patients: 9822\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 0: curr_epoch_loss=0.5222952961921692\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 1: curr_epoch_loss=0.4214431643486023\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 2: curr_epoch_loss=0.37444984912872314\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 3: curr_epoch_loss=0.3423888385295868\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 4: curr_epoch_loss=0.3379351794719696\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 5: curr_epoch_loss=0.3232121169567108\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 6: curr_epoch_loss=0.32139480113983154\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 7: curr_epoch_loss=0.30863896012306213\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 8: curr_epoch_loss=0.2972787022590637\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 9: curr_epoch_loss=0.28879880905151367\n",
      "Model Training time: 189.85386109352112\n",
      "Learning rate: 0.001\n",
      "Model Training time: 189.85386109352112\n",
      "Precision =  0.625\n",
      "Recall    =  0.0199203187250996\n",
      "F1        =  0.03861003861003861\n",
      "ROC AUC   =  0.7826500299850773\n",
      "0.625 \t 0.0199203187250996 \t 0.03861003861003861 \t 0.7826500299850773\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.625, 0.0199203187250996, 0.03861003861003861, 0.7826500299850773)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Original model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_unbalanced_dataloaders()   # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'unbalanced_model.pt')\n",
    "#load_and_eval(model, 'unbalanced_model.pt', val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 0.0001\n",
      "Number of Epochs: 10\n",
      "\n",
      "\n",
      "--------------\n",
      "Balanced model\n",
      "--------------\n",
      "* Train dataset *\n",
      "Number of Patients: 13790\n",
      "* Test dataset *\n",
      "Number of Patients: 1965\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 0: curr_epoch_loss=0.6428765654563904\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 1: curr_epoch_loss=0.5184852480888367\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 2: curr_epoch_loss=0.40095871686935425\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 3: curr_epoch_loss=0.3157050907611847\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 4: curr_epoch_loss=0.2823430597782135\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 5: curr_epoch_loss=0.24877764284610748\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 6: curr_epoch_loss=0.19739803671836853\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 7: curr_epoch_loss=0.16388234496116638\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 8: curr_epoch_loss=0.17176353931427002\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 9: curr_epoch_loss=0.14679090678691864\n",
      "Model Training time: 338.0585379600525\n",
      "Learning rate: 0.0001\n",
      "Model Training time: 338.0585379600525\n",
      "Precision =  0.7026022304832714\n",
      "Recall    =  0.8076923076923077\n",
      "F1        =  0.7514910536779325\n",
      "ROC AUC   =  0.9377589160951381\n",
      "0.7026022304832714 \t 0.8076923076923077 \t 0.7514910536779325 \t 0.9377589160951381\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7026022304832714,\n",
       " 0.8076923076923077,\n",
       " 0.7514910536779325,\n",
       " 0.9377589160951381)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "print ('')\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Balanced model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_balanced_dataloaders()       # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'balanced_model.pt')\n",
    "#load_and_eval(model, 'balanced_model.pt', val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}