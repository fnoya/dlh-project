{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inititalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1060 6GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to set what is the maximum number of visit dates for any given patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_max_visits = 505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need a function to load the pre-processed train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patient_mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, notes_mask, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        self.x = patients_notes.to(device, non_blocking=True)\n",
    "        self.notes_mask = notes_mask.to(device, non_blocking=True)\n",
    "        self.y = mortality.to(device, non_blocking=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "              \n",
    "\n",
    "        return(self.x[index].to_dense(), self.notes_mask[index].to_dense(), self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (cohort_type = 'original'):\n",
    "    \"\"\"\n",
    "    cohort_type = 'original' -> Unbalanced cohort will be created\n",
    "    cohort_type = 'balanced_train' -> Balanced cohort for training will be created\n",
    "    cohort_type = 'balanced_test' -> Balanced cohort for testing will be created\n",
    "    \"\"\"\n",
    "    notes_prefix = \"orig_\" if cohort_type == 'original' else \"train_\" if cohort_type == 'balanced_train' else \"test_\"\n",
    "    subject_id, patients_notes_fetures, index_0, index_1, patient_mortality= load_notes_dataset_object(prefix = notes_prefix)\n",
    "    index = [index_0, index_1]\n",
    "    patients_notes_fetures = torch.sparse_coo_tensor(index, patients_notes_fetures, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    ones = np.ones((len(index_0),200))\n",
    "    notes_mask = torch.sparse_coo_tensor(index, ones, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    patient_mortality = torch.from_numpy(patient_mortality).float()    \n",
    "    \n",
    "    \n",
    "    dataset = NotesDataset(subject_id, patients_notes_fetures, notes_mask, patient_mortality)\n",
    "    assert len(patient_mortality) == len(dataset), 'Wrong dataset length!'\n",
    "    print (\"Number of Patients:\", len(patient_mortality))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbalanced_dataloaders (max_size = 0):\n",
    "\n",
    "    dataset = create_dataset('original')\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        dataset = Subset(dataset, np.arange(max_size))\n",
    "\n",
    "    split = int(len(dataset)*0.8)\n",
    "    lengths = [split, len(dataset) - split]\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_dataloaders (max_size = 0):\n",
    "\n",
    "    print (\"* Train dataset *\")\n",
    "    balanced_train_dataset = create_dataset('balanced_train')\n",
    "    print (\"* Test dataset *\")\n",
    "    balanced_test_dataset = create_dataset('balanced_test')\n",
    "\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        balanced_train_dataset = Subset(balanced_train_dataset, np.arange(max_size))\n",
    "        balanced_test_dataset = Subset(balanced_test_dataset, np.arange(max_size))\n",
    "\n",
    "    train_loader = DataLoader(balanced_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(balanced_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Network Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesAlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\"\n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = self.a_att(g)\n",
    "        alpha = torch.softmax(weights,1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesBetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        weights = self.b_att(h)\n",
    "        beta = torch.tanh(weights)\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesRNN(nn.Module):\n",
    "    \n",
    "    def attention_sum(self, alpha, beta, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "            beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.sum( x * alpha * beta * masks , dim=1 )\n",
    "    \n",
    "    def __init__(self, hidden_dim=128, notes_emb_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_a = nn.GRU(notes_emb_size, notes_emb_size, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(notes_emb_size, notes_emb_size, batch_first=True)\n",
    "        self.att_a = NotesAlphaAttention(notes_emb_size)\n",
    "        self.att_b = NotesBetaAttention(notes_emb_size)\n",
    "        self.fc = nn.Linear(notes_emb_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        g, _ = self.rnn_a(x)\n",
    "        h, _ = self.rnn_b(x)\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        c = self.attention_sum(alpha, beta, x, masks)\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "\n",
    "        return probs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer():\n",
    "    model = NotesRNN(hidden_dim=128)\n",
    "    if torch.cuda.device_count() >0:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, nesterov = True)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "            x, masks, labels = batch\n",
    "        \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(x, masks)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks)\n",
    "            val_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().cpu().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, val_loader, n_epochs=10, filename='model.pt'):\n",
    "    t0 = time.time()\n",
    "    train(model, train_loader, n_epochs)\n",
    "    t1 = time.time()\n",
    "    processing_time = t1-t0\n",
    "    print('Model Training time: ' + str(processing_time))\n",
    "    \n",
    "    p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print (\"Learning rate: \" + str(learning_rate))\n",
    "    print(\"Model Training time: \" + str(processing_time))\n",
    "    print(\"Precision = \",p)\n",
    "    print(\"Recall    = \", r)\n",
    "    print(\"F1        = \", f)\n",
    "    print(\"ROC AUC   = \", roc_auc)\n",
    "    print(p,\"\\t\",r,\"\\t\",f,\"\\t\",roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\n",
      "Number of Epochs: 20\n",
      "\n",
      "--------------\n",
      "Original model\n",
      "--------------\n",
      "Number of Patients: 9822\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 0: curr_epoch_loss=0.6112962365150452\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 1: curr_epoch_loss=0.4883677363395691\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 2: curr_epoch_loss=0.42417651414871216\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 3: curr_epoch_loss=0.37340787053108215\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 4: curr_epoch_loss=0.3304714262485504\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 5: curr_epoch_loss=0.30555108189582825\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 6: curr_epoch_loss=0.29249006509780884\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 7: curr_epoch_loss=0.26100945472717285\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 8: curr_epoch_loss=0.23929330706596375\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 9: curr_epoch_loss=0.2203543484210968\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 10: curr_epoch_loss=0.2148643136024475\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 11: curr_epoch_loss=0.21104291081428528\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 12: curr_epoch_loss=0.1900014877319336\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 13: curr_epoch_loss=0.1710783988237381\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 14: curr_epoch_loss=0.1497410535812378\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 15: curr_epoch_loss=0.15790478885173798\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 16: curr_epoch_loss=0.15124185383319855\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 17: curr_epoch_loss=0.13597123324871063\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 18: curr_epoch_loss=0.12872570753097534\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, Epoch 19: curr_epoch_loss=0.12475761026144028\n",
      "Model Training time: 372.80315613746643\n",
      "Learning rate: 0.0001\n",
      "Model Training time: 372.80315613746643\n",
      "Precision =  0.44324324324324327\n",
      "Recall    =  0.33064516129032256\n",
      "F1        =  0.3787528868360277\n",
      "ROC AUC   =  0.7245382982321001\n",
      "0.44324324324324327 \t 0.33064516129032256 \t 0.3787528868360277 \t 0.7245382982321001\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_epochs = 20\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Original model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_unbalanced_dataloaders()   # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'unbalanced_model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\n",
      "Number of Epochs: 10\n",
      "\n",
      "\n",
      "--------------\n",
      "Balanced model\n",
      "--------------\n",
      "* Train dataset *\n",
      "Number of Patients: 13790\n",
      "* Test dataset *\n",
      "Number of Patients: 1965\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 0: curr_epoch_loss=0.6445119976997375\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 1: curr_epoch_loss=0.49357670545578003\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 2: curr_epoch_loss=0.3861011266708374\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 3: curr_epoch_loss=0.30750030279159546\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 4: curr_epoch_loss=0.27264195680618286\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 5: curr_epoch_loss=0.23895473778247833\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 6: curr_epoch_loss=0.19071021676063538\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 7: curr_epoch_loss=0.17378121614456177\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 8: curr_epoch_loss=0.18220755457878113\n",
      "Batch : 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, Epoch 9: curr_epoch_loss=0.17062851786613464\n",
      "Model Training time: 327.69331669807434\n",
      "Learning rate: 0.0001\n",
      "Model Training time: 327.69331669807434\n",
      "Precision =  0.7235772357723578\n",
      "Recall    =  0.7606837606837606\n",
      "F1        =  0.7416666666666666\n",
      "ROC AUC   =  0.919079925145783\n",
      "0.7235772357723578 \t 0.7606837606837606 \t 0.7416666666666666 \t 0.919079925145783\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "print (\"Number of Epochs: \" + str(n_epochs))\n",
    "print ('')\n",
    "print ('')\n",
    "print ('--------------')\n",
    "print ('Balanced model')\n",
    "print ('--------------')\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train_loader, val_loader = get_balanced_dataloaders()       # You can pass a number to limit the number of samples\n",
    "train_and_eval(model, train_loader, val_loader, n_epochs, 'balanced_model.pt')\n",
    "#load_and_eval(model, 'balanced_model.pt', val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
