{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd0372d86228f3adce91ee8f73488429c59d9cef194793ee1ea2efae707e6a5484f",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notes_events_network_join.ipynb"
   ]
  },
  {
   "source": [
    "# Hyperparameters experiment\n",
    "We train and eval the model varying the following:\n",
    "\n",
    "* cohorts: unbalanced vs balanced\n",
    "* optimizers: Adam vs SGD\n",
    "* learning rates\n",
    "* epochs\n",
    "* number of samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = ['unbalanced', 'balanced']\n",
    "optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "epochs = [5, 10, 15]\n",
    "samples = [1000, 5000, 0]\n",
    "\n",
    "results = np.empty(shape=(len(cohorts), len(optimizers), len(learning_rates), len(epochs), len(samples)), dtype='object')\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for c, cohort in enumerate(cohorts):\n",
    "    for o, optim in enumerate(optimizers):\n",
    "        for l, learning_rate in enumerate(learning_rates):\n",
    "            for e, n_epochs in enumerate(epochs):\n",
    "                for s, n_samples in enumerate(samples):\n",
    "                    model, optimizer = create_model_and_optimizer()\n",
    "                    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "                    print (\"Training for:\\n\")\n",
    "                    print (\"Cohort        \\t= \", cohort)\n",
    "                    print (\"Optimizer     \\t= \", optimizer)\n",
    "                    print (\"Learning rate \\t= \", learning_rate)\n",
    "                    print (\"No. of epochs \\t= \", n_epochs)\n",
    "                    print (\"No. of samples\\t= \", n_samples)\n",
    "                    print ('---------------')\n",
    "                    model_filename = cohort + '-' + str(o) + '-' + str(learning_rate) + '-' + str(n_epochs) + '-' + str(n_samples) + '.pt'\n",
    "\n",
    "                    train_loader, val_loader = get_unbalanced_dataloaders(n_samples) if cohort=='unbalanced' else get_balanced_dataloaders(n_samples)                    \n",
    "                    p, r, f, roc_auc = train_and_eval(model, train_loader, val_loader, n_epochs, model_filename)\n",
    "                    results[c,o,l,e,s] = [p, r, f, roc_auc]\n",
    "                    print ('---------------\\n\\n')\n"
   ]
  },
  {
   "source": [
    "## Saving of the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "r = {}\n",
    "r['cohorts'] = cohorts\n",
    "r['optimizers'] = optimizers\n",
    "r['learning_rates'] = learning_rates\n",
    "r['epochs'] = epochs\n",
    "r['samples'] = samples\n",
    "r['results'] = results\n",
    "pickle.dump( r, open(\"hyperparam-exp-\" + now.strftime(\"%Y%m%d-%H%M\") +\".p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cohorts': ['unbalanced', 'balanced'],\n",
       " 'optimizers': [torch.optim.adam.Adam, torch.optim.sgd.SGD],\n",
       " 'learning_rates': [0.0001, 0.001, 0.01],\n",
       " 'epochs': [5, 10, 15],\n",
       " 'samples': [100, 1000, 0],\n",
       " 'results': array([[[[[list([0.0, 0.0, 0.0, 0.42105263157894735]),\n",
       "            list([0.0, 0.0, 0.0, 0.6039391993149219]),\n",
       "            list([0.5957446808510638, 0.47257383966244726, 0.5270588235294118, 0.8747509376465074])],\n",
       "           [list([0.0, 0.0, 0.0, 0.1111111111111111]),\n",
       "            list([1.0, 0.029411764705882353, 0.05714285714285715, 0.8027994330262225]),\n",
       "            list([0.5958904109589042, 0.3625, 0.4507772020725389, 0.8672004830917873])],\n",
       "           [list([0.0, 0.0, 0.0, 0.1111111111111111]),\n",
       "            list([0.9, 0.3, 0.45000000000000007, 0.8535294117647059]),\n",
       "            list([0.6304347826086957, 0.48333333333333334, 0.5471698113207547, 0.8656666666666667])]],\n",
       " \n",
       "          [[list([0.0, 0.0, 0.0, 0.26315789473684215]),\n",
       "            list([0.8571428571428571, 0.24, 0.375, 0.8267428571428572]),\n",
       "            list([0.7612903225806451, 0.46825396825396826, 0.5798525798525798, 0.8943420528359233])],\n",
       "           [list([0.0, 0.0, 0.0, 0.368421052631579]),\n",
       "            list([0.5625, 0.391304347826087, 0.46153846153846156, 0.8437730287398674]),\n",
       "            list([0.6233766233766234, 0.6180257510729614, 0.6206896551724138, 0.8893957715905598])],\n",
       "           [list([0.0, 0.0, 0.0, 0.631578947368421]),\n",
       "            list([0.5263157894736842, 0.4, 0.45454545454545453, 0.8187428571428571]),\n",
       "            list([0.7337662337662337, 0.48917748917748916, 0.5870129870129869, 0.8833890062263765])]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]]],\n",
       " \n",
       " \n",
       "         [[[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]]],\n",
       " \n",
       " \n",
       "         [[[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]],\n",
       " \n",
       "          [[None, None, None],\n",
       "           [None, None, None],\n",
       "           [None, None, None]]]]], dtype=object)}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}