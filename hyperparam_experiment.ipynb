{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd0372d86228f3adce91ee8f73488429c59d9cef194793ee1ea2efae707e6a5484f",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notes_events_network_join.ipynb"
   ]
  },
  {
   "source": [
    "# Hyperparameters experiment\n",
    "We train and eval the model varying the following:\n",
    "\n",
    "* cohorts: unbalanced vs balanced\n",
    "* optimizers: Adam vs SGD\n",
    "* learning rates\n",
    "* epochs\n",
    "* number of samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = ['unbalanced', 'balanced']\n",
    "optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "epochs = [5, 10, 15]\n",
    "samples = [100, 1000, 0]\n",
    "\n",
    "results = np.empty(shape=(len(cohorts), len(optimizers), len(learning_rates), len(epochs), len(samples)), dtype='object')\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for c, cohort in enumerate(cohorts):\n",
    "    for o, optim in enumerate(optimizers):\n",
    "        for l, learning_rate in enumerate(learning_rates):\n",
    "            for e, n_epochs in enumerate(epochs):\n",
    "                for s, n_samples in enumerate(samples):\n",
    "                    model, optimizer = create_model_and_optimizer()\n",
    "                    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "                    print (\"Training for:\\n\")\n",
    "                    print (\"Cohort        \\t= \", cohort)\n",
    "                    print (\"Optimizer     \\t= \", optimizer)\n",
    "                    print (\"Learning rate \\t= \", learning_rate)\n",
    "                    print (\"No. of epochs \\t= \", n_epochs)\n",
    "                    print (\"No. of samples\\t= \", n_samples)\n",
    "                    print ('---------------')\n",
    "                    model_filename = cohort + '-' + str(o) + '-' + str(learning_rate) + '-' + str(n_epochs) + '-' + str(n_samples) + '.pt'\n",
    "\n",
    "                    train_loader, val_loader = get_unbalanced_dataloaders(n_samples) if cohort=='unbalanced' else get_balanced_dataloaders(n_samples)                    \n",
    "                    p, r, f, roc_auc = train_and_eval(model, train_loader, val_loader, n_epochs, model_filename)\n",
    "                    results[c,o,l,e,s] = [p, r, f, roc_auc]\n",
    "                    print ('---------------\\n\\n')\n"
   ]
  },
  {
   "source": [
    "## Saving of the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "r = {}\n",
    "r['cohorts'] = cohorts\n",
    "r['optimizers'] = optimizers\n",
    "r['learning_rates'] = learning_rates\n",
    "r['epochs'] = epochs\n",
    "r['samples'] = samples\n",
    "r['results'] = results\n",
    "pickle.dump( r, open(\"hyperparam-exp-\" + now.strftime(\"%Y%m%d-%H%M\") +\".p\", \"wb\" ) )"
   ]
  },
  {
   "source": [
    "## Plotting the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}