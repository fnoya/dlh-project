{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-deployment",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "concerned-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string\n",
    "# import nltk\n",
    "#from nltk import word_tokenize\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import boto3\n",
    "#from botocore.client import ClientError\n",
    "# below is used to print out pretty pandas dataframes\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "#from pyathena import connect\n",
    "#from pyathena.pandas.util import as_pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mysterious-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "PATIENTS = 0  # 0 - ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-distribution",
   "metadata": {},
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to find what is the maximum number of visit dates for any given patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aboriginal-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "patients_max_visits = 505\n",
    "print(patients_max_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-anthropology",
   "metadata": {},
   "source": [
    "In preparation to run the models training on CUDA, we need to make sure that we do have a device and load the tensors and model to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "congressional-piece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-bibliography",
   "metadata": {},
   "source": [
    "We will need a function to load the pre-processed train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gothic-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patients_notes_last_date = np.load(prefix + 'patients_notes_last_date.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "    \n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patients_notes_last_date, patient_mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-spouse",
   "metadata": {},
   "source": [
    "We now load the objects we for train and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "derived-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_items = pickle.load( open( \"events_item.p\", \"rb\" ) )\n",
    "events_values = pickle.load(open(\"events_value.p\", \"rb\") )\n",
    "patients = pickle.load(open('patients.p', 'rb'))\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb')) + 1\n",
    "\n",
    "assert len(events_items)==174288 and len(events_values)==174288 and len(events_values['subject_id'].unique()) == len(events_items['subject_id'].unique()) == len(patients) == 9822, \"Wrong events dataframes?\"\n",
    "assert max_code==127, \"EVENTS MAX CODE changed?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "contained-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_subject_id, orig_patients_notes_fetures, orig_index_0, orig_index_1, orig_patients_notes_last_date, orig_patient_mortality = load_notes_dataset_object(prefix = 'orig_')\n",
    "orig_index = [orig_index_0, orig_index_1]\n",
    "orig_patients_notes_fetures = torch.sparse_coo_tensor(orig_index, orig_patients_notes_fetures, (len(orig_subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "orig_patients_notes_last_date = torch.from_numpy(orig_patients_notes_last_date).long()\n",
    "orig_patient_mortality = torch.from_numpy(orig_patient_mortality).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-syria",
   "metadata": {},
   "source": [
    "Now we are going to create a custom notes dataset to then partition the data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "appointed-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NotesEventsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, last_date_idx, events_items, events_values, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        len_events_patients = len(events_items['subject_id'].unique())\n",
    "        if torch.cuda.device_count() >0:\n",
    "            self.x = patients_notes.to(device, non_blocking=True)\n",
    "            self.mask = last_date_idx.to(device, non_blocking=True)\n",
    "            self.y = mortality.to(device, non_blocking=True)\n",
    "            self.items = events_items.groupby('subject_id').agg('codes').apply(list).values.to(device, non_blocking=True)\n",
    "            self.values = events_values.groupby('subject_id').agg('values').apply(list).values.to(device, non_blocking=True)            \n",
    "        else:\n",
    "            self.x = patients_notes\n",
    "            self.mask = last_date_idx\n",
    "            self.y = mortality\n",
    "            self.items = events_items.groupby('subject_id').agg('codes').apply(list).values\n",
    "            self.values = events_values.groupby('subject_id').agg('values').apply(list).values\n",
    "        assert len(self.x) == len_events_patients, 'Notes patients and events patients counts do not match!'\n",
    "        r = random.randrange(len(self.x))\n",
    "        assert events_items['subject_id'].unique()[r] == self.patient_id[r], 'Notes and events patient id=' + str(r) + ' does not match'\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        events = np.zeros([len(self.items[index]), max_code])\n",
    "\n",
    "        for i, codes in enumerate(self.items[index]):\n",
    "            for j, code in enumerate(codes):\n",
    "                v = self.values[index][i][j]\n",
    "                events[i, code] = v if not math.isnan(v) else 0.0\n",
    "        \n",
    "        return(self.patient_id , self.x[index].to_dense(), self.mask[index], events, self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "accessible-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients: 9822\n",
      "Len of dataset: 9822\n"
     ]
    }
   ],
   "source": [
    "notes_orig_dataset = NotesEventsDataset(orig_subject_id, orig_patients_notes_fetures, orig_patients_notes_last_date, events_items, events_values, orig_patient_mortality)\n",
    "print (\"Patients:\", len(patients))\n",
    "print (\"Len of dataset:\", len(notes_orig_dataset))\n",
    "assert len(patients) == len(notes_orig_dataset), 'Wrong dataset length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-commonwealth",
   "metadata": {},
   "source": [
    "Now we create the data loaders, splitting the dataset on 80% train, 20% validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "committed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    subject_id, x, notes_mask, events, mortality_flag = zip(*data)\n",
    "    \n",
    "    maxvisits = max([len(p) for p in events])\n",
    "    \n",
    "    events_result = torch.tensor([np.concatenate((p, np.zeros([maxvisits - len(p), max_code]))) for p in events]).float()\n",
    "    events_mask = torch.tensor([np.concatenate((np.ones(len(p)), np.zeros(maxvisits - len(p)))) for p in events]).int()\n",
    "    \n",
    "    return torch.tensor(subject_id).int(), x, notes_mask, events_result, events_mask, mortality_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "available-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(230729)\n",
    "split = int(len(notes_orig_dataset)*0.8)\n",
    "lengths = [split, len(notes_orig_dataset) - split]\n",
    "\n",
    "notes_orig_train_dataset, notes_orig_val_dataset = random_split(notes_orig_dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "massive-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 50\n",
    "notes_orig_train_loader = DataLoader(notes_orig_train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "notes_orig_val_loader = DataLoader(notes_orig_val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-skirt",
   "metadata": {},
   "source": [
    "Now We can proceed to create our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "subtle-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, notes_emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_size = notes_emb_size\n",
    "        self.RNN = nn.GRU(input_size = notes_emb_size, hidden_size = notes_emb_size, batch_first = True)\n",
    "        self.fc1 = nn.Linear(notes_emb_size,notes_emb_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(notes_emb_size,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, step):\n",
    "                \n",
    "        rnn_out = self.RNN(x)\n",
    "        last_note_date_hs = get_last_note_date(rnn_out[0],masks)\n",
    "        fc1_out = self.fc1(last_note_date_hs)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        dp_out = self.dropout(fc1_out)\n",
    "        fc2_out = self.fc2(dp_out)\n",
    "        out = self.sig(fc2_out).flatten()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-desktop",
   "metadata": {},
   "source": [
    "Since the number of date_notes is not the same for each patient, we need to get the hidden state for the last note date for each patient, for that we implement the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "personalized-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_note_date(hidden_states, masks):   \n",
    "    #last_visit = ((masks.sum(axis = 2) > 0).sum(axis = 1) - 1).unsqueeze(-1)\n",
    "    #if(step == 134):\n",
    "    #print(masks)\n",
    "    #print(hidden_states.shape)\n",
    "    last_visit = masks.expand(-1,hidden_states.shape[2]).unsqueeze(1)\n",
    "    \n",
    "    out = torch.gather(hidden_states,dim = 1,index = last_visit)[:,-1,:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "standing-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_volume(W, K, S, P):\n",
    "    \n",
    "\n",
    "    \n",
    "    return  (((W-K+2*P)//S)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "grand-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "                #print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "            subject_id, x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(x, masks, step)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "obvious-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        subject_id, x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks,0)\n",
    "            val_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().cpu().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "steady-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "notes_rnn = NotesRNN(notes_emb_size = 200)\n",
    "if torch.cuda.device_count() >0:\n",
    "    notes_rnn.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(notes_rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "absent-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\n",
      "Batch : "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-8f27b0172968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learning Rate: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotes_orig_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotes_orig_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprocessing_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-fe9a3505abe8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m\"\"\" Step 2. evaluate model ouput  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;34m\"\"\" Step 3. Calculate loss  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-fb4d12f0de3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, masks, step)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mrnn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlast_note_date_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_note_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfc1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_note_date_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "t0 = time.time()\n",
    "print('Learning Rate: ' + str(learning_rate))\n",
    "n_epochs = 5\n",
    "train(notes_rnn, notes_orig_train_loader, notes_orig_val_loader, n_epochs)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Model Training time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stupid-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7209302325581395 0.14418604651162792 0.24031007751937988 0.7779495016611296\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(notes_rnn, notes_orig_val_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-texas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
