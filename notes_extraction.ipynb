{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.client import ClientError\n",
    "# below is used to print out pretty pandas dataframes\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from pyathena import connect\n",
    "from pyathena.pandas.util import as_pandas\n",
    "import time\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to enable us to import data from athena services, be able to query parquet files through SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "athena_query_results_bucket = 'aws-athena-query-results-'+account_id+'-'+region\n",
    "\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=athena_query_results_bucket)\n",
    "except ClientError:\n",
    "    bucket = s3.create_bucket(Bucket=athena_query_results_bucket)\n",
    "    print('Creating bucket '+athena_query_results_bucket)\n",
    "cursor = connect(s3_staging_dir='s3://'+athena_query_results_bucket+'/athena/temp').cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and process notes for Cohort Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Notes from Cohort Patients within observation window:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a table with the patients clinical notes only for those patients that we are considering as part of our cohort, also, any notes from patients that are within the last 48 hrs before last discharge are not considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ('CREATE TABLE default.diabetic_patients_notes WITH (format=''PARQUET'') AS '\n",
    "         'select cohort.subject_id, nts.hadm_id, (CASE WHEN nts.charttime IS NOT NULL THEN nts.charttime ELSE nts.chartdate END) as charttime, cohort.mortality_flag, nts.text '\n",
    "         'FROM  default.diabetic_patients_cohort cohort '\n",
    "         'left outer join mimiciii.noteevents as nts '\n",
    "         'on nts.subject_id = cohort.subject_id '\n",
    "         'AND (CASE WHEN nts.charttime IS NOT NULL THEN nts.charttime ELSE nts.chartdate END) >= (CASE WHEN nts.charttime IS NOT NULL THEN cohort.admit_time ELSE date(cohort.admit_time) END) '\n",
    "         'AND (CASE WHEN nts.charttime IS NOT NULL THEN nts.charttime ELSE nts.chartdate END) <= (CASE WHEN nts.charttime IS NOT NULL THEN date_add(''hour'',-48,cohort.discharge_time) '\n",
    "                                                                                                                                      'ELSE date(date_add(''hour'',-48, cohort.discharge_time)) END) '\n",
    "         'ORDER BY cohort.subject_id ASC, (CASE WHEN nts.charttime IS NOT NULL THEN nts.charttime ELSE nts.chartdate END) ASC')\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Notes by distinct date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Clinical Notes can be scattered accross different dates, but also, a single date can have more than one note, we will aggregate the notes that are in a single date, we will do this by concatenating all the notes of a single date into a single string column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ('CREATE TABLE default.diabetic_patients_notes_agg WITH (format=''PARQUET'') AS '\n",
    "         'select nts.subject_id,  date(charttime) as chart_date, array_join(array_agg( nts.text || ' || '), '') AS notes_agg '\n",
    "         'from default.diabetic_patients_notes nts '\n",
    "         'group by nts.subject_id, date(charttime) '\n",
    "         'order by nts.subject_id asc, date(charttime) asc ')\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Notes Embeding for each distinct Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is to pre-process the notes, get rid of numbers, punctuation and tokenize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):    \n",
    "    ''' Preprocess the text data. And return a list of clinical notes. '''\n",
    "    clinical_notes = []\n",
    "    \n",
    "    df.notes_agg = df.notes_agg.fillna(' ')  # remove NA\n",
    "    df.notes_agg = df.notes_agg.str.replace('\\n',' ')  # remove newline\n",
    "    df.notes_agg = df.notes_agg.str.replace('\\r',' ')\n",
    "    \"\"\"\n",
    "    TODO: 1. remove punc;\n",
    "          2. remove numbers.\n",
    "          \n",
    "    HINT: consider using `string.punctuation`, `str.maketrans`, and `str.translate`.\n",
    "    \"\"\"\n",
    "    df.notes_agg = df.notes_agg.str.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    df.notes_agg = df.notes_agg.str.translate(str.maketrans('', '', '1234567890')) # remove numbers\n",
    "    \n",
    "    df.notes_agg = df.notes_agg.str.lower()  # convert to lower case\n",
    "    \n",
    "    # tokenize\n",
    "    for note in df.notes_agg.values:\n",
    "        note_tokenized = word_tokenize(note)\n",
    "        clinical_notes.append(note_tokenized)\n",
    "\n",
    "    return clinical_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process our notes, we first need to load our pre-trained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "#pubMedWord2VecModel = KeyedVectors.load_word2vec_format('PubMed-w2v.bin', binary=True)\n",
    "word2vec_model = KeyedVectors.load('note_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a function to process the datasets (train or tests and create the neccesary objects that later we will use on the notes network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(cohort_patients_df, patients_date_notes_df, patients_max_visits):\n",
    "    patient_subject_id = []\n",
    "    \n",
    "    #sparse vector values\n",
    "    patients_notes_fetures = []\n",
    "    #sparse vector indexes\n",
    "    index_0 = []\n",
    "    index_1 = []\n",
    "    # Patients last note_date, sql query returns the max number of vistis, but in pythong, we index starting with zero\n",
    "    # therefore we need to substract 1, but for those patients with zero notes (we still need to keep thos for cosnistency with events network),\n",
    "    # we need to make thos as the max note being 0 (zero indexed)\n",
    "    zero_notes = np.array(patients_max_visits.number_dates.values) - 1 == -1\n",
    "    patients_notes_last_date = np.array(patients_max_visits.number_dates.values) - 1\n",
    "    patients_notes_last_date[zero_notes] = 0\n",
    "    patients_notes_last_date = np.expand_dims(patients_notes_last_date, axis=1)\n",
    "    missing_words = 0\n",
    "    # Targe label\n",
    "    patient_mortality = np.array(cohort_patients_df.mortality_flag.values)\n",
    "    \n",
    "\n",
    "    for patient_idx, patient in enumerate(cohort_patients_df.subject_id.values):\n",
    "        if patient_idx % 100 == 0 and patient_idx >0:\n",
    "            print('Processing patient ' + str(patient_idx))\n",
    "        patient_subject_id.append(patient)\n",
    "        patient_date_notes_list = preprocess_dataset(patients_date_notes_df[patients_date_notes_df.subject_id == patient].copy())\n",
    "        #print(patient_date_notes_list)\n",
    "        for date_idx, note in enumerate(patient_date_notes_list):\n",
    "            patient_date_note = np.zeros(200, dtype =  np.float64)\n",
    "            #print(note)\n",
    "            for note_word in note:\n",
    "                #if note_word not in vocab:\n",
    "                    #vocab.append(note_word)\n",
    "                try:\n",
    "                    patient_date_note = patient_date_note + word2vec_model.get_vector(note_word)\n",
    "                except:\n",
    "                    missing_words = missing_words + 1\n",
    "                    #if note_word not in missing_words:\n",
    "                        #missing_words =  missing_words+1\n",
    "            #if patient_idx == 0:\n",
    "                #print(patient_date_note)\n",
    "            index_0.append(patient_idx)\n",
    "            index_1.append(date_idx)\n",
    "            patients_notes_fetures.append(patient_date_note)\n",
    "            \n",
    "\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patients_notes_last_date, patient_mortality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are to help us save and re-load all the processed notes into/from file objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_notes_dataset_objects(patient_subject_id, patients_notes_fetures, index_0, index_1, patients_notes_last_date, patient_mortality, prefix = ''):\n",
    "    \n",
    "    np.save(prefix + 'subject_id.npy', patient_subject_id, allow_pickle=True)\n",
    "    np.save(prefix + 'patients_notes_fetures.npy', patients_notes_fetures, allow_pickle=True)\n",
    "    np.save(prefix + 'index_0.npy', index_0, allow_pickle=True)\n",
    "    np.save(prefix + 'index_1.npy', index_1, allow_pickle=True)\n",
    "    np.save(prefix + 'patients_notes_last_date.npy', patients_notes_last_date, allow_pickle=True)\n",
    "    np.save(prefix + 'patient_mortality.npy', patient_mortality, allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patients_notes_last_date = np.load(prefix + 'patients_notes_last_date.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patients_notes_last_date, patient_mortality\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Cohort, Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already pre-processed our cohort sets using SQL queries, the following is the query to fetch the list of all test cohort patients and then process the notes for each patient, we first do it for the train set, then for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select cohort.new_subject_id as subject_id, cohort.mortality_flag from default.train_cohort2 cohort order by cohort.new_subject_id'\n",
    "cursor.execute(query)\n",
    "cohort_patients_df = as_pandas(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to find what is the maximum number of visit dates for any given patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ('select cohort.new_subject_id as subject_id, count(nts.chart_date) as number_dates '\n",
    "         'from default.diabetic_patients_notes_agg nts join default.train_cohort2 cohort '\n",
    "         'on nts.subject_id = cohort.subject_id group by cohort.new_subject_id order by cohort.new_subject_id;')\n",
    "cursor.execute(query)\n",
    "patients_max_visits = as_pandas(cursor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "query = ('select cohort.subject_id, nts.chart_date, nts.notes_agg from default.diabetic_patients_notes_agg nts '\n",
    "         'join (select cohort.new_subject_id as subject_id, cohort.subject_id as old_subject_id from default.train_cohort2 cohort order by cohort.new_subject_id) cohort '\n",
    "         'on nts.subject_id = cohort.old_subject_id '\n",
    "         'order by cohort.subject_id asc, nts.chart_date asc;')\n",
    "cursor.execute(query)\n",
    "patients_date_notes_df = as_pandas(cursor)\n",
    "print('Total number of train patients_dates: ' +str(len(patients_date_notes_df)))\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Training set notes Agg query time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "train_subject_id, train_patients_notes_fetures, train_index_0, train_index_1, train_patients_notes_last_date, train_patient_mortality = process_dataset(cohort_patients_df, patients_date_notes_df, patients_max_visits)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Training set processing time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients_notes_last_date = np.expand_dims(train_patients_notes_last_date, axis=1)\n",
    "save_notes_dataset_objects(train_subject_id, train_patients_notes_fetures, train_index_0, train_index_1, train_patients_notes_last_date, train_patient_mortality, prefix = 'train_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Cohort, test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now process the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select cohort.subject_id as subject_id, cohort.mortality_flag from default.test_cohort cohort order by cohort.subject_id'\n",
    "cursor.execute(query)\n",
    "cohort_patients_df = as_pandas(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "query = ('select cohort.subject_id, nts.chart_date, nts.notes_agg from default.diabetic_patients_notes_agg nts '\n",
    "         'join (select cohort.subject_id from default.test_cohort cohort order by cohort.subject_id) cohort '\n",
    "         'on nts.subject_id = cohort.subject_id '\n",
    "         'order by cohort.subject_id asc, nts.chart_date asc;')\n",
    "cursor.execute(query)\n",
    "patients_date_notes_df = as_pandas(cursor)\n",
    "print('Total number of train patients_dates: ' +str(len(patients_date_notes_df)))\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Test set notes Agg query time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ('select cohort.subject_id, count(nts.chart_date) as number_dates '\n",
    "         'from default.diabetic_patients_notes_agg nts join default.test_cohort cohort '\n",
    "         'on nts.subject_id = cohort.subject_id group by cohort.subject_id order by cohort.subject_id;')\n",
    "cursor.execute(query)\n",
    "patients_max_visits = as_pandas(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "test_subject_id, test_patients_notes_fetures, test_index_0, test_index_1, test_patients_notes_last_date, test_patient_mortality = process_dataset(cohort_patients_df, patients_date_notes_df,patients_max_visits)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Training set processing time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_notes_dataset_objects(test_subject_id, test_patients_notes_fetures, test_index_0, test_index_1, test_patients_notes_last_date, test_patient_mortality, prefix = 'test_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subject_id_cp, train_patients_notes_fetures_cp, train_index_0_cp, train_index_1_cp, train_patients_notes_last_date, train_patient_mortality = load_notes_dataset_object(prefix = 'train_') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unblanced Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will process the Unbalanced Cohort, no need to have test and train as this split will be done later on the main Network notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select cohort.subject_id as subject_id, cohort.mortality_flag from default.diabetic_patients_cohort cohort order by cohort.subject_id'\n",
    "cursor.execute(query)\n",
    "cohort_patients_df = as_pandas(cursor)\n",
    "cohort_patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "query = ('select nts.subject_id, nts.chart_date, nts.notes_agg from default.diabetic_patients_notes_agg nts '\n",
    "         'order by nts.subject_id asc, nts.chart_date asc;')\n",
    "cursor.execute(query)\n",
    "patients_date_notes_df = as_pandas(cursor)\n",
    "print('Total number of train patients_dates: ' +str(len(patients_date_notes_df)))\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Original set notes Agg query time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ('select nts.subject_id as subject_id, count(nts.chart_date) as number_dates '\n",
    "         'from default.diabetic_patients_notes_agg nts '\n",
    "         'group by nts.subject_id order by nts.subject_id;')\n",
    "cursor.execute(query)\n",
    "patients_max_visits = as_pandas(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "orig_subject_id, orig_patients_notes_fetures, orig_index_0, orig_index_1, orig_patients_notes_last_date, orig_patient_mortality = process_dataset(cohort_patients_df, patients_date_notes_df,patients_max_visits)\n",
    "t1 = time.time()\n",
    "processing_time = t1-t0\n",
    "print('Original set processing time: ' + str(processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_notes_dataset_objects(orig_subject_id, orig_patients_notes_fetures, orig_index_0, orig_index_1, orig_patients_notes_last_date, orig_patient_mortality, prefix = 'orig_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
