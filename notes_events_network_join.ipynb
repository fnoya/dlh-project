{
 "cells": [
  {
   "source": [
    "# Datasets, Dataloaders and Main model objects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\programdata\\anaconda3\\lib\\site-packages (5.0.8)\n",
      "Requirement already satisfied: traitlets>=4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat) (5.0.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat) (4.6.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat) (3.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat) (227)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat) (20.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat) (50.3.1.post20201107)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb')) + 1\n",
    "assert max_code==127, \"EVENTS MAX CODE changed?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that not all patients have the same number of visit dates, therefore, we need to find what is the maximum number of visit dates for any given patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_max_visits = 505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation to run the models training on CUDA, we need to make sure that we do have a device and load the tensors and model to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1060 6GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from files\n",
    "\n",
    "These files were generated from SQL queries contained in notes_extraction.ipynb and events_extraction.ipynb notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events_dataset_object(prefix=''):\n",
    "    return pickle.load( open(prefix + \"events_item.p\", \"rb\" )), pickle.load(open(prefix + \"events_value.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notes_dataset_object(prefix = ''):\n",
    "    \n",
    "    patient_subject_id = np.load(prefix + 'subject_id.npy', allow_pickle=True).tolist()\n",
    "    patients_notes_fetures = np.load(prefix + 'patients_notes_fetures.npy', allow_pickle=True)\n",
    "    index_0 = np.load(prefix + 'index_0.npy', allow_pickle=True)\n",
    "    index_1 = np.load(prefix + 'index_1.npy', allow_pickle=True)\n",
    "    patient_mortality = np.load(prefix + 'patient_mortality.npy', allow_pickle=True)\n",
    "\n",
    "    return patient_subject_id, patients_notes_fetures, index_0, index_1, patient_mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesEventsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_id, patients_notes, notes_mask, events_items, events_values, mortality):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        len_events_patients = len(events_items['subject_id'].unique())\n",
    "        self.x = patients_notes.to(device, non_blocking=True)\n",
    "        self.notes_mask = notes_mask.to(device, non_blocking=True)\n",
    "        self.y = mortality.to(device, non_blocking=True)\n",
    "        self.items = events_items.groupby('subject_id').agg('codes').apply(list).values\n",
    "        self.values = events_values.groupby('subject_id').agg('values').apply(list).values\n",
    "        assert len(self.x) == len_events_patients, 'Notes patients and events patients counts do not match!'\n",
    "        r = random.randrange(len(self.x))\n",
    "        assert events_items['subject_id'].unique()[r] == self.patient_id[r], 'Notes and events patient id=' + str(r) + ' does not match'\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        events = np.zeros([len(self.items[index]), max_code])\n",
    "\n",
    "        for i, codes in enumerate(self.items[index]):\n",
    "            for j, code in enumerate(codes):\n",
    "                v = self.values[index][i][j]\n",
    "                events[i, code] = v if not math.isnan(v) else 0.0\n",
    "        \n",
    "\n",
    "        return(self.x[index].to_dense(), self.notes_mask[index].to_dense(), events, self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (cohort_type = 'original'):\n",
    "    \"\"\"\n",
    "    cohort_type = 'original' -> Unbalanced cohort will be created\n",
    "    cohort_type = 'essential'  ->  Unbalanced cohort with just the minimum set of events features\n",
    "    cohort_type = 'balanced_train' -> Balanced cohort for training will be created\n",
    "    cohort_type = 'balanced_test' -> Balanced cohort for testing will be created\n",
    "    \"\"\"\n",
    "    notes_prefix = \"orig_\" if cohort_type in ['original','essential'] else \"train_\" if cohort_type == 'balanced_train' else \"test_\"\n",
    "    subject_id, patients_notes_fetures, index_0, index_1, patient_mortality= load_notes_dataset_object(prefix = notes_prefix)\n",
    "    index = [index_0, index_1]\n",
    "    patients_notes_fetures = torch.sparse_coo_tensor(index, patients_notes_fetures, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    ones = np.ones((len(index_0),200))\n",
    "    notes_mask = torch.sparse_coo_tensor(index, ones, (len(subject_id),patients_max_visits,200), dtype = torch.float)\n",
    "    \n",
    "    patient_mortality = torch.from_numpy(patient_mortality).float()    \n",
    "    \n",
    "    events_prefix = \"\" if cohort_type=='original' else cohort_type +\"_\" \n",
    "    events_items, events_values = load_events_dataset_object(events_prefix)\n",
    "\n",
    "    assert len(events_items)==len(events_values) and len(events_values['subject_id'].unique()) == len(events_items['subject_id'].unique()) == len(patient_mortality) , \"Wrong events dataframes?\"\n",
    "        \n",
    "    dataset = NotesEventsDataset(subject_id, patients_notes_fetures, notes_mask, events_items, events_values, patient_mortality)\n",
    "    assert len(patient_mortality) == len(dataset), 'Wrong dataset length!'\n",
    "    print (\"Number of Patients:\", len(patient_mortality))\n",
    "\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    x, notes_mask, events, mortality_flag = zip(*data)\n",
    "    \n",
    "    maxvisits = max([len(p) for p in events])\n",
    "    \n",
    "    events_result = torch.tensor([np.concatenate((p, np.zeros([maxvisits - len(p), max_code]))) for p in events]).float()\n",
    "    events_mask = torch.tensor([np.concatenate((np.ones(len(p)), np.zeros(maxvisits - len(p)))) for p in events]).int()\n",
    "    x = torch.stack(x)\n",
    "    notes_mask = torch.stack(notes_mask)\n",
    "    mortality_flag = torch.stack(mortality_flag)\n",
    "    events_result = events_result.to(device, non_blocking=True)\n",
    "    events_mask = events_mask.to(device, non_blocking=True)\n",
    "    return x, notes_mask, events_result, events_mask, mortality_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the data loaders, splitting the dataset on 80% train, 20% validation."
   ]
  },
  {
   "source": [
    "### Dataloader for the unbalanced (original) cohort."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbalanced_dataloaders (max_size = 0):\n",
    "\n",
    "    dataset = create_dataset('original')\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        dataset = Subset(dataset, np.arange(max_size))\n",
    "\n",
    "    split = int(len(dataset)*0.8)\n",
    "    lengths = [split, len(dataset) - split]\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "source": [
    "### Dataloader for the unbalanced cohort but with the reduced (essential) features set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essential_dataloaders (max_size = 0):\n",
    "\n",
    "    dataset = create_dataset('essential')\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        dataset = Subset(dataset, np.arange(max_size))\n",
    "\n",
    "    split = int(len(dataset)*0.8)\n",
    "    lengths = [split, len(dataset) - split]\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "source": [
    "### Dataloader for the balanced cohort (data up-sampling) for \"dead\" patients to eliminate class imbalance)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_dataloaders (max_size = 0):\n",
    "\n",
    "    print (\"* Train dataset *\")\n",
    "    balanced_train_dataset = create_dataset('balanced_train')\n",
    "    print (\"* Test dataset *\")\n",
    "    balanced_test_dataset = create_dataset('balanced_test')\n",
    "\n",
    "    if (max_size > 0):\n",
    "        print (\"***** Slicing to \" + str(max_size))\n",
    "        balanced_train_dataset = Subset(balanced_train_dataset, np.arange(max_size))\n",
    "        balanced_test_dataset = Subset(balanced_test_dataset, np.arange(max_size))\n",
    "\n",
    "    train_loader = DataLoader(balanced_train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(balanced_test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notes_module.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run events_module.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classifier Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutNet(nn.Module):\n",
    "    def __init__(self, notes_embeddings=128, events_embeddings=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.notes = NotesRNN(notes_embeddings)\n",
    "        self.events = EventsRNN(max_code, emb_size=events_embeddings)\n",
    "        if torch.cuda.device_count() >0:\n",
    "            self.notes.cuda()\n",
    "            self.events.cuda()\n",
    "        self.fc1 = nn.Linear(notes_embeddings + events_embeddings, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sig = nn.Sigmoid()    \n",
    "    \n",
    "    def forward(self, x, masks, events, events_masks):\n",
    "        \n",
    "        \n",
    "        events_emb = self.events(events, events_masks)\n",
    "        \n",
    "        notes_emb = self.notes(x, masks)\n",
    "        #notes_emb = torch.rand(events_emb.shape[0],128)\n",
    "        #events_emb = torch.rand(notes_emb.shape[0], 128)\n",
    "        \n",
    "        \n",
    "        joint_emb = torch.cat([notes_emb, events_emb], dim=1)\n",
    "        \n",
    "        fc1 = self.fc1(joint_emb)\n",
    "        fc2 = self.dropout(self.fc2(fc1))\n",
    "        fc3 = self.fc3(fc2)\n",
    "        output = self.sig(fc3).flatten()\n",
    "        return output\n",
    "\n",
    "#outNet = OutNet()\n",
    "#outNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer():\n",
    "    model = OutNet(notes_embeddings=128, events_embeddings=128)\n",
    "    if torch.cuda.device_count() >0:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, nesterov = True)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        print('Batch :', end = ' ')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 10 == 0 and step>0:\n",
    "                print(str(step)+',', end=' ' )\n",
    "            x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" Step 2. evaluate model ouput  \"\"\"\n",
    "            probs = model(x, masks, events, events_masks)\n",
    "            \"\"\" Step 3. Calculate loss  \"\"\"\n",
    "            loss = criterion(probs, labels)\n",
    "            \"\"\" Step 4. Backward propagation  \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, events, events_masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks, events, events_masks)\n",
    "            val_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            val_probs.extend(probs.detach().cpu().numpy().reshape(-1).tolist())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs)>0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, val_loader, n_epochs=10, filename='model.pt'):\n",
    "    t0 = time.time()\n",
    "    train(model, train_loader, n_epochs)\n",
    "    t1 = time.time()\n",
    "    processing_time = t1-t0\n",
    "    print('Model Training time: ' + str(processing_time))\n",
    "    \n",
    "    p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print (\"Learning rate: \" + str(learning_rate))\n",
    "    print(\"Model Training time: \" + str(processing_time))\n",
    "    print(\"Precision = \",p)\n",
    "    print(\"Recall    = \", r)\n",
    "    print(\"F1        = \", f)\n",
    "    print(\"ROC AUC   = \", roc_auc)\n",
    "    print(p,\"\\t\",r,\"\\t\",f,\"\\t\",roc_auc)\n",
    "    \n",
    "    if filename is not None:\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_eval(model, modelfilename, val_loader):  \n",
    "    print (\"**LOADING MODEL** from \" + modelfilename)\n",
    "    model.load_state_dict(torch.load(modelfilename, map_location=torch.device('cpu')))\n",
    "    p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "    print (\"\")\n",
    "    print(\"Precision = \",p)\n",
    "    print(\"Recall    = \", r)\n",
    "    print(\"F1        = \", f)\n",
    "    print(\"ROC AUC   = \", roc_auc)\n",
    "    print(p,\"\\t\",r,\"\\t\",f,\"\\t\",roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python368jvsc74a57bd0372d86228f3adce91ee8f73488429c59d9cef194793ee1ea2efae707e6a5484f",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "372d86228f3adce91ee8f73488429c59d9cef194793ee1ea2efae707e6a5484f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}