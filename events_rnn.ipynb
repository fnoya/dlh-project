{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "juvenile-manchester",
   "metadata": {},
   "source": [
    "# RNN for events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imposed-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "events_items = pickle.load( open( \"events_item.p\", \"rb\" ) )\n",
    "events_values = pickle.load(open(\"events_value.p\", \"rb\") )\n",
    "patients = pickle.load(open('patients.p', 'rb'))\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb'))\n",
    "\n",
    "assert len(events_items)==174272 and len(events_values)==174272 and len(patients)==9822, \"Wrong dataframes?\"\n",
    "assert max_code==126, \"MAX CODE changed?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "soviet-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-opening",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "broad-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patients, events_items, events_values):\n",
    "\n",
    "        self.patients = events_items['subject_id'].unique()\n",
    "        self.y = patients\n",
    "        self.items = events_items.groupby('subject_id').agg('codes').apply(list).values\n",
    "        self.values = events_values.groupby('subject_id').agg('values').apply(list).values\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the number of patients.\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.patients)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \n",
    "        Outputs:\n",
    "            - subject_id\n",
    "            - tensor of visits, multi-hot items values\n",
    "            - mortality flag\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        events = np.zeros([len(self.items[index]), max_code])\n",
    "\n",
    "        for i, codes in enumerate(self.items[index]):\n",
    "            for j, code in enumerate(codes):\n",
    "                v = self.values[index][i][j]\n",
    "                events[i, code] = v if not math.isnan(v) else 0.0\n",
    "        \n",
    "        subject_id = int(self.y[self.y['subject_id']==self.patients[index]]['subject_id'])\n",
    "        mortality_flag = int(self.y[self.y['subject_id']==self.patients[index]]['mortality_flag'])\n",
    "        \n",
    "        return subject_id, events, mortality_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "annual-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(patients, events_items, events_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dirty-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    subject_id, events, mortality_flag = zip(*data)\n",
    "    \n",
    "    maxvisits = max([len(p) for p in events])\n",
    "    \n",
    "    result = torch.tensor([np.concatenate((p, np.zeros([maxvisits - len(p), max_code]))) for p in events]).long()\n",
    "    mask = torch.tensor([np.concatenate((np.ones(len(p)), np.zeros(maxvisits - len(p)))) for p in events]).int()\n",
    "    \n",
    "    return torch.tensor(subject_id).int(), result, mask, torch.tensor(mortality_flag).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eight-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "subjects, events, masks, y = next(loader_iter)\n",
    "\n",
    "assert subjects.shape==torch.Size([10]) and events.shape==torch.Size([10,34,126]) and masks.shape==torch.Size([10,34]) and y.shape==torch.Size([10]), \"Wrong dimensions!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "vital-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 7844\n",
      "Length of val dataset: 1962\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "worthy-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)    \n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-meditation",
   "metadata": {},
   "source": [
    "#### Naive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes=max_code, emb_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_codes, emb_size)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size=emb_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(emb_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Pass the sequence through the embedding layer;\n",
    "            2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "               Use `sum_embeddings_with_mask()`;\n",
    "            3. Pass the embegginds through the RNN layer;\n",
    "            4. Obtain the hidden state at the last visit.\n",
    "               Use `get_last_visit()`;\n",
    "            5. Pass the hidden state through the linear and activation layers.\n",
    "            \n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "            \n",
    "        Note that rev_x, rev_masks are passed in as arguments so that we can use the same \n",
    "        training and validation function for both models. You can ignore the them here.\n",
    "        \"\"\"\n",
    "        \n",
    "        emb = self.embedding(x)\n",
    "        #print (\"emb=\",emb.shape)\n",
    "        \n",
    "        visits_emb = sum_embeddings_with_mask(emb, masks)\n",
    "        #print (\"visits_emb=\",visits_emb.shape)\n",
    "        rnn_hidden_states = self.rnn(visits_emb)\n",
    "        #print (\"rnn_hidden_states= len:\", len(rnn_hidden_states), \" element0:\", rnn_hidden_states[0].shape, \" element1:\", rnn_hidden_states[1].shape)\n",
    "\n",
    "        last_visit_hidden_state = get_last_visit(rnn_hidden_states[0], masks)\n",
    "        #print (\"last_visit_hidden_state= \", last_visit_hidden_state.shape)\n",
    "        fc1 = self.fc1(last_visit_hidden_state)\n",
    "        #print (\"fc1= \", fc1.shape)\n",
    "        output = self.sig(fc1).flatten()\n",
    "        #print (\"output= \", output.shape)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(types))\n",
    "naive_rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
