{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accomplished-hindu",
   "metadata": {},
   "source": [
    "# RNN for events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ranking-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "events_items = pickle.load( open( \"events_item.p\", \"rb\" ) )\n",
    "events_values = pickle.load(open(\"events_value.p\", \"rb\") )\n",
    "patients = pickle.load(open('patients.p', 'rb'))\n",
    "max_code = pickle.load(open('events_maxcode.p', 'rb')) + 1\n",
    "\n",
    "assert len(events_items)==174272 and len(events_values)==174272 and len(patients)==9822, \"Wrong dataframes?\"\n",
    "assert max_code==127, \"MAX CODE changed?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "previous-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 230729\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "PATIENTS = 10000  # 0 - ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-desperate",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "heated-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patients, events_items, events_values):\n",
    "\n",
    "        self.patients = events_items['subject_id'].unique()\n",
    "        self.y = patients\n",
    "        self.items = events_items.groupby('subject_id').agg('codes').apply(list).values\n",
    "        self.values = events_values.groupby('subject_id').agg('values').apply(list).values\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the number of patients.\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.patients)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \n",
    "        Outputs:\n",
    "            - subject_id\n",
    "            - tensor of visits, multi-hot items values\n",
    "            - mortality flag\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        events = np.zeros([len(self.items[index]), max_code])\n",
    "\n",
    "        for i, codes in enumerate(self.items[index]):\n",
    "            for j, code in enumerate(codes):\n",
    "                v = self.values[index][i][j]\n",
    "                events[i, code] = v if not math.isnan(v) else 0.0\n",
    "        \n",
    "        subject_id = int(self.y[self.y['subject_id']==self.patients[index]]['subject_id'])\n",
    "        mortality_flag = int(self.y[self.y['subject_id']==self.patients[index]]['mortality_flag'])\n",
    "        \n",
    "        return subject_id, events, mortality_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "future-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients: 9822\n",
      "Len of dataset: 9806\n"
     ]
    }
   ],
   "source": [
    "if PATIENTS > 0 :\n",
    "    patients = patients[:PATIENTS]\n",
    "\n",
    "events_items = events_items[events_items['subject_id'].isin(patients['subject_id'])]\n",
    "events_values = events_values[events_values['subject_id'].isin(patients['subject_id'])]\n",
    "dataset = CustomDataset(patients, events_items, events_values)\n",
    "\n",
    "print (\"Patients:\", len(patients))\n",
    "print (\"Len of dataset:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "hazardous-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    subject_id, events, mortality_flag = zip(*data)\n",
    "    \n",
    "    maxvisits = max([len(p) for p in events])\n",
    "    \n",
    "    result = torch.tensor([np.concatenate((p, np.zeros([maxvisits - len(p), max_code]))) for p in events]).float()\n",
    "    mask = torch.tensor([np.concatenate((np.ones(len(p)), np.zeros(maxvisits - len(p)))) for p in events]).int()\n",
    "    \n",
    "    return torch.tensor(subject_id).int(), result, mask, torch.tensor(mortality_flag).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "central-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "subjects, events, masks, y = next(loader_iter)\n",
    "\n",
    "#assert subjects.shape==torch.Size([10]) and events.shape==torch.Size([10,34,126]) and masks.shape==torch.Size([10,34]) and y.shape==torch.Size([10]), \"Wrong dimensions!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "comprehensive-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 7844\n",
      "Length of val dataset: 1962\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "czech-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)    \n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-wings",
   "metadata": {},
   "source": [
    "#### Naive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "growing-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (rnn): GRU(127, 128, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes=max_code, emb_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "       # self.embedding = nn.Embedding(num_codes, emb_size)\n",
    "        self.rnn = nn.GRU(num_codes, hidden_size=emb_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(emb_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, events, masks):\n",
    "        \n",
    "        rnn_hidden_states, _ = self.rnn(events)\n",
    "        \n",
    "        real_hidden_states = rnn_hidden_states * masks.unsqueeze(-1).expand(rnn_hidden_states.shape)\n",
    "        \n",
    "        sum_hidden_states = real_hidden_states.sum(dim=1)\n",
    "\n",
    "        fc1 = self.fc1(sum_hidden_states)\n",
    "        output = self.sig(fc1).flatten()\n",
    "        return output\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN()\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-shelter",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "herbal-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "hollywood-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):    \n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for subjects, x, masks, y in val_loader:\n",
    "        outputs = model(x, masks)\n",
    "        Y_pred.append(outputs.detach().numpy())\n",
    "        Y_test.append(y.numpy())\n",
    "    \n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "    roc_auc = roc_auc_score(Y_test, Y_pred)\n",
    "    Y_pred[ Y_pred < 0.5 ] = 0\n",
    "    Y_pred[ Y_pred >= 0.5 ] = 1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(Y_test, Y_pred, average='binary')\n",
    "\n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "featured-measure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: precision=0.5360824742268041 recall=0.22510822510822512, f1=0.3170731707317073, roc_auc=0.6239393189133223\n",
      "Epoch 1: precision=0.5289256198347108 recall=0.27705627705627706, f1=0.36363636363636365, roc_auc=0.7423854789539365\n",
      "Epoch 2: precision=0.5294117647058824 recall=0.38961038961038963, f1=0.4488778054862843, roc_auc=0.7797672191086402\n",
      "Epoch 3: precision=0.46308724832214765 recall=0.5974025974025974, f1=0.5217391304347827, roc_auc=0.8328219056122004\n",
      "Epoch 4: precision=0.624 recall=0.33766233766233766, f1=0.4382022471910112, roc_auc=0.7695024020847244\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for subjects, events, masks, target in train_loader:\n",
    "            # your code here\n",
    "            optimizer.zero_grad()\n",
    "            output = model(events, masks)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        precision, recall, f1, roc_auc = eval_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch}: precision={precision} recall={recall}, f1={f1}, roc_auc={roc_auc}\")\n",
    "    \n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-episode",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
